Human-AI collaboration is the study of how humans and artificial intelligence (AI) agents work together to accomplish a shared goal. AI systems can aid humans in everything from decision making tasks to art creation. Examples of collaboration include medical decision making aids., hate speech detection, and music generation. As AI systems are able to tackle more complex tasks, studies are exploring how different models and explanation techniques can improve human-AI collaboration.

When a human uses an AI's output, they often want to understand why a model gave a certain output. While some models, like decision trees, are inherently explainable, black box models do not have clear explanations. Various Explainable artificial intelligence methods aim to describe model outputs with post-hoc explanations or visualizations, these methods can often provide misleading and false explanations. Studies have also found that explanations may not improve the performance of a human-AI team, but simply increase a human's reliance on the model's output.

A human's trust in an AI agent is an important factor in human-AI collaboration, dictating whether the human should follow or override the AI's input. Various factors impact a person's trust in an AI system, including its accuracy and reliability.